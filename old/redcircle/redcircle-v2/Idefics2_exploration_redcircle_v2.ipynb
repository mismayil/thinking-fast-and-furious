{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d587931-23c6-4aba-9db3-9aaca6b2055f",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73d13d-f066-43a0-9e67-f4f1301efcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate datasets peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b0985-8fa0-486f-ac67-7ec2638c6108",
   "metadata": {},
   "source": [
    "### Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294860c-9ff8-4295-9d88-bd60026b5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "with open(\"/home/cchang/wandb.key\", \"r\") as f:\n",
    "    wandb_key = f.read().strip()\n",
    "    \n",
    "wandb.login(key=wandb_key)\n",
    "wandb.init(\n",
    "    project=\"DriveLM\",\n",
    ")\n",
    "wandb_name=wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220712f0-2373-459a-ba34-137696e94ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f7044a9-b085-4c3f-bdd1-4be9e6e71910",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/samples'\n",
    "IMAGE_PATH_PREFIX = '../nuscenes/samples'\n",
    "IMAGE_SRC_X, IMAGE_SRC_Y = 1600, 900\n",
    "IMAGE_TGT_X, IMAGE_TGT_Y = int(IMAGE_SRC_X / 2.5), int(IMAGE_SRC_Y / 2.5)\n",
    "\n",
    "train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/v1_1_train_nus_ext.json'\n",
    "test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/drivelm/challenge/test_eval.json'\n",
    "idefics_train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-v2/data/nuscenes/train_v1_ext_idefics_redcircle_v2.json'\n",
    "idefics_test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-v2/data/nuscenes/test_ext_idefics_redcircle_v2.json'\n",
    "zero_shot_predictions_path = \"/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-v2/outputs/test-eval-idefics2-8b-zero-shot_redcirecle_v2.json\"\n",
    "# checkpoint_dir = f\"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/models/idefics_redcircle/{wandb_name}\"\n",
    "checkpoint_dir = \"/mnt/nlpdata1/home/ismayilz/cs503-project/models/idefics2-redcircle-prime-music-8-500step/checkpoint-500\"\n",
    "# checkpoint_dir = \"/mnt/nlpdata1/home/ismayilz/cs503-project/models/idefics2-redcircle-fearless-morning-10-1650step/checkpoint-1650\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b81418b8-2fc6-43a3-94e2-5d3b520dd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:23<00:00, 11.92s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    do_image_splitting=False\n",
    ")\n",
    "\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        # \"HuggingFaceM4/idefics2-8b\",\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "    # model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "else:\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        # \"HuggingFaceM4/idefics2-8b\",\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\", # Only available on A100 or H100\n",
    "    ).to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f6e0d44-7fe7-4ab5-9738-47b0c9630988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "MCQ_QUESTION = \"Please select the correct answer from the following options:\"\n",
    "BEHAVIOR_FEW_SHOT_PROMPT = \"\"\"{question} {mcq_question}\n",
    "A. This is an example behavior 1. \n",
    "B. This is an example behavior 2. \n",
    "C. This is an example behavior 3.\n",
    "D. This is an example behavior 4.\n",
    "Answer: B\n",
    "\"\"\"\n",
    "def vizualize_frames(image_paths):\n",
    "    y_view_mapping = {\"MIDDLE\": 1, \"LEFT\": 0, \"RIGHT\": 2}\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(48, 18))\n",
    "    for i, (image_view, image_path) in enumerate(image_paths.items()):\n",
    "        # image = Image.open(image_path)\n",
    "        image=copy.deepcopy(image_path)\n",
    "        _, x, y = f\"{image_view}_MIDDLE\".split(\"_\")[:3]\n",
    "        x_id = int(x == 'BACK')\n",
    "        axes[x_id][y_view_mapping[y]].imshow(image)\n",
    "        axes[x_id][y_view_mapping[y]].set_title(image_view)\n",
    "        axes[x_id][y_view_mapping[y]].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def process_scene(scene_id, scene):\n",
    "    samples = []\n",
    "    for frame_id, frame in scene['key_frames'].items():\n",
    "        image_paths = {view_name: view_path.replace(IMAGE_PATH_PREFIX, IMAGE_DIR) for view_name, view_path in frame['image_paths'].items()}\n",
    "        assert len(image_paths) == 6, \"not all views provided\"\n",
    "        question_id = 0\n",
    "        for question_type, questions in frame['QA'].items():\n",
    "            for question_info in questions:\n",
    "                question = question_info['Q']\n",
    "                answer = question_info['A'] if \"A\" in question_info else \"\"\n",
    "                sample_id = f\"{scene_id}_{frame_id}_{question_id}\"\n",
    "                question_id += 1\n",
    "                question_text = question\n",
    "                if 0 in question_info[\"tag\"] and MCQ_QUESTION in question_text:\n",
    "                    actual_q, options = question_text.split(MCQ_QUESTION)\n",
    "                    options = options.replace(\"A.\", \"\\nA.\").replace(\"B.\", \"\\nB.\").replace(\"C.\", \"\\nC.\").replace(\"D.\", \"\\nD.\")\n",
    "                    question_text = f\"{BEHAVIOR_FEW_SHOT_PROMPT.format(question=actual_q.strip(), mcq_question=MCQ_QUESTION)}\\n{actual_q.strip()} {MCQ_QUESTION}\\n{options.strip()}\\nAnswer:\"\n",
    "                samples.append({\n",
    "                    \"id\": sample_id, #change key here from sample_id to id\n",
    "                    \"question_type\": question_type,\n",
    "                    \"question_text\": question_text,\n",
    "                    \"images\": image_paths,\n",
    "                    \"answer\": answer,\n",
    "                    \"tag\": question_info[\"tag\"]\n",
    "                })\n",
    "    return samples\n",
    "\n",
    "\n",
    "def process_dataset(data_path, output_path=None):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        dataset: Dict[str, str] = json.load(f)\n",
    "    samples = []\n",
    "    for scene_id, scene in tqdm(dataset.items()):\n",
    "        samples.extend(process_scene(scene_id, scene))\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(samples, f, indent=4)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def convert_sample_to_idefics(sample):\n",
    "    idefics_sample = copy.deepcopy(sample)\n",
    "    question = sample[\"question_text\"]\n",
    "    user_message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK_LEFT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK_RIGHT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT_LEFT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT_RIGHT\"},\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    idefics_sample[\"user_message\"] = user_message\n",
    "    return idefics_sample\n",
    "\n",
    "\n",
    "def produce_idefics_dataset(samples, output_path=None):\n",
    "    idefics_samples = []\n",
    "    for sample in samples:\n",
    "        idefics_samples.append(convert_sample_to_idefics(sample))\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(idefics_samples, f, indent=4)\n",
    "            \n",
    "    return idefics_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c1997c8-a985-425a-805d-fe32d0f5c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def objects_to_dict(question):\n",
    "    # get all objects in the question\n",
    "    objects = re.findall(r'<[^>]*>', question)\n",
    "    unique_objects = list(set(objects))\n",
    "    result = {}\n",
    "    for obj in unique_objects:\n",
    "        # Remove '<' and '>' and split by comma\n",
    "        parts = obj.strip('<>').split(',')\n",
    "        # The identifier seems to be the second element based on your example\n",
    "        identifier = parts[1]\n",
    "        # Coordinates are the last two elements\n",
    "        coordinates = [float(parts[2]), float(parts[3])]\n",
    "        # Check if the identifier already exists in the dictionary\n",
    "        if identifier in result:\n",
    "            # Append the new coordinates to the existing list\n",
    "            result[identifier].append(coordinates)\n",
    "        else:\n",
    "            # Otherwise, create a new list with the coordinates\n",
    "            result[identifier] = [coordinates]\n",
    "    # result will look like {'CAM_BACK': [[1088.3, 497.5]], 'CAM_FRONT': [[1043.2, 82.2]]}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d5c38c6-797b-4bd1-b90d-24856c0bdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_red_circle(image_path,image_key, objects):\n",
    "    image=load_image(image_path)\n",
    "    if image_key in objects.keys() and bool(objects):\n",
    "        for coordinate in objects[image_key]:\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            # Define the radius of the circle and the color\n",
    "            # Base on paper: we draw red circles over the images, with radius r = 0.06H and thickness t = 0.01H, where H is the shorter side of the image.\n",
    "            H= min(image.size)\n",
    "            radius = 0.06 * H\n",
    "            thickness = 0.01 * H\n",
    "            x = float(coordinate[0])\n",
    "            y = float(coordinate[1])\n",
    "            # Calculate the bounding box of the circle to be drawn\n",
    "            left_up_point = (int(x - radius), int(y - radius))\n",
    "            right_down_point = (int(x + radius), int(y + radius))\n",
    "            draw.ellipse([left_up_point, right_down_point], outline='red',fill=None, width=int(thickness))\n",
    "            #for checking center\n",
    "            # radius_center=10\n",
    "            # left_up_point = (int(x - radius_center), int(y - radius_center))\n",
    "            # right_down_point = (int(x + radius_center), int(y + radius_center))\n",
    "            # draw.ellipse([left_up_point, right_down_point],fill='blue')\n",
    "\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "deaabb30-7c12-435a-b8a8-88978fbdad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_for_viz(image_paths,images):\n",
    "    for i,key in enumerate(image_paths.keys()):\n",
    "        image_paths[key]=images[i]\n",
    "    return image_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5a2ef9b-cdaf-44cf-9818-db8fbf26ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_set, verbose=False):\n",
    "    predictions = []\n",
    "    for idefics_sample in tqdm(test_set):\n",
    "        if 0 in idefics_sample[\"tag\"] and MCQ_QUESTION in idefics_sample[\"question_text\"]:\n",
    "            image_paths = idefics_sample['images']\n",
    "            objects=objects_to_dict(idefics_sample['question_text'])\n",
    "            # images = [load_image(image_path).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_path in image_paths.values()]\n",
    "            images = [draw_red_circle(image_paths[image_key], image_key, objects).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_key in image_paths.keys()]\n",
    "            if verbose:\n",
    "                image_viz=construct_for_viz(copy.deepcopy(image_paths),images)\n",
    "                vizualize_frames(image_viz)\n",
    "                print('objects:',objects)\n",
    "        \n",
    "            prompt = processor.apply_chat_template(idefics_sample['user_message'], add_generation_prompt=True)\n",
    "            inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "            # Generate\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "            predicted_text = generated_texts[0].split('\\n')[-1][len(\"Assistant: \"):]\n",
    "            prediction = copy.deepcopy(idefics_sample)\n",
    "            prediction['gt'] = prediction['answer']\n",
    "            prediction['answer'] = predicted_text\n",
    "            predictions.append(prediction)\n",
    "            if verbose:\n",
    "                print(idefics_sample['question_text'])\n",
    "                print('Predicted:', predicted_text)\n",
    "                print('GT:', prediction['gt'])\n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18709197-47be-45de-967a-9dd4e50d2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 299.29it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = process_dataset(train_data_path)\n",
    "test_dataset = process_dataset(test_data_path)\n",
    "\n",
    "# train_idefics_dataset = produce_idefics_dataset(train_dataset, output_path=idefics_train_data_path)\n",
    "test_idefics_dataset = produce_idefics_dataset(test_dataset, output_path=idefics_test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cca1dc-a1da-4b69-b377-6926279ffe7a",
   "metadata": {},
   "source": [
    "### Zero-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a29ff74-3313-4ab7-90aa-536226f90bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [02:07<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_predictions = eval_model(model, test_idefics_dataset)\n",
    "\n",
    "# zero_shot_predictions = eval_model(model, test_idefics_dataset)\n",
    "# with open(zero_shot_predictions_path, \"w\") as f:\n",
    "#      json.dump(zero_shot_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "205539c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../redcircle/outputs/test-eval-idefics2-8b-fine-tuned-redcircle-prime-music-8-500step.json\") as f:\n",
    "    orig_results = json.load(f)\n",
    "\n",
    "merged_results = []\n",
    "new_results_map = {}\n",
    "\n",
    "for res in zero_shot_predictions:\n",
    "    new_results_map[res[\"id\"]] = res\n",
    "\n",
    "for res in orig_results:\n",
    "    new_res = new_results_map.get(res[\"id\"])\n",
    "\n",
    "    if new_res:\n",
    "        new_res[\"raw_answer\"] = new_res[\"answer\"]\n",
    "        new_res[\"answer\"] = new_res[\"answer\"].strip()[0]\n",
    "        merged_results.append(new_res)\n",
    "    else:\n",
    "        merged_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1122271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/test-eval-idefics2-8b-fine-tuned-redcircle-prime-music-8-500step-one-shot.json\", \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5d1d7-1422-415c-a6cb-18e189b1d8a3",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940c40-3e4e-4049-b61f-12026f1e613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #new change to so that it's different scenes in train and test\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.model_selection import train_test_split \n",
    "\n",
    "# data = load_dataset('json', data_files=idefics_train_data_path, split=None)\n",
    "# scene_ids = list(set(data['train']['scene_id']))\n",
    "# train_ids, val_ids = train_test_split(scene_ids, test_size=0.025)\n",
    "# train_dataset = data['train'].filter(lambda x: x['scene_id'] in train_ids)\n",
    "# val_dataset = data['train'].filter(lambda x: x['scene_id'] in val_ids)\n",
    "# print(f'Train set: {len(train_dataset)}, Val set: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8d49f-e80e-44c9-9d05-894d9eaefa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('json', data_files=idefics_train_data_path, split=None)\n",
    "split_dataset = data['train'].train_test_split(test_size=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db613a3-f72b-49ed-9559-a6347b97305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVQADataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            objects=objects_to_dict(example['question_text'])\n",
    "            sample_images = [draw_red_circle(example['images'][image_key], image_key, objects).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_key in example['images'].keys()]\n",
    "\n",
    "            # #for make sure red circle is there\n",
    "            # image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\n",
    "            # vizualize_frames(image_viz)\n",
    "            # print('Question:',)\n",
    "            # print('objects:',objects)\n",
    "            \n",
    "            answer_text = example[\"answer\"]\n",
    "            answer_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": answer_text}\n",
    "                    ]\n",
    "            }\n",
    "            user_message = example['user_message'][0]\n",
    "            messages = [user_message, answer_message]\n",
    "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "            texts.append(text.strip())\n",
    "            images.append(sample_images)\n",
    "\n",
    "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.image_token_id\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c74f84-afd4-4840-8a19-c7e291181152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=500,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    output_dir=checkpoint_dir,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=75,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"idefics label fix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabdabd-41e5-4e0a-9b93-11b55419a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = GVQADataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"], # You can also evaluate (loss) on the eval set, note that it will incur some additional GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a0c10-9cfe-4471-9438-593c3ea8171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc0ee3-d7da-48b0-9ddc-cab15346b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs\n",
    "finetuned_model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/models/idefics_redcircle/wandb_name/checkpoint-500\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config if USE_QLORA else None,\n",
    ")\n",
    "_=eval_model(finetuned_model, test_idefics_dataset[:10], verbose=True)\n",
    "fine_tune_predictions_path = f\"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/outputs/test-eval-idefics2-8b-fine-tunned-{wandb_name}_redcirecle.json\"\n",
    "fine_tune_predictions = eval_model(model, test_idefics_dataset)\n",
    "with open(fine_tune_predictions_path, \"w\") as f:\n",
    "     json.dump(fine_tune_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8971f4-df3d-4c67-803c-55abc03d9d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
