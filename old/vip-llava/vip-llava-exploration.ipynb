{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/nlpdata1/home/ismayilz/.wandb.key\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     wandb_key \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "with open(\"/mnt/nlpdata1/home/ismayilz/.wandb.key\", \"r\") as f:\n",
    "    wandb_key = f.read().strip()\n",
    "    \n",
    "wandb.login(key=wandb_key)\n",
    "wandb.init(\n",
    "    project=\"thinking-fast-and-furious\",\n",
    ")\n",
    "wandb_name=wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VipLlavaForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "USE_LORA = False\n",
    "USE_QLORA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/samples'\n",
    "IMAGE_PATH_PREFIX = '../nuscenes/samples'\n",
    "IMAGE_SRC_X, IMAGE_SRC_Y = 1600, 900\n",
    "IMAGE_TGT_X, IMAGE_TGT_Y = int(IMAGE_SRC_X / 2.5), int(IMAGE_SRC_Y / 2.5)\n",
    "\n",
    "train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/v1_1_train_nus_ext.json'\n",
    "test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/drivelm/challenge/test_eval.json'\n",
    "vipllava_train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/vip-llava/data/nuscenes/train_vip_llava.json'\n",
    "vipllava_test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/vip-llava/data/nuscenes/test_vip_llava.json'\n",
    "# checkpoint_dir = f\"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/models/idefics_redcircle/{wandb_name}\"\n",
    "# checkpoint_dir = \"/mnt/nlpdata1/home/ismayilz/cs503-project/models/idefics2-redcircle-prime-music-8-500step/checkpoint-500\"\n",
    "# checkpoint_dir = \"/mnt/nlpdata1/home/ismayilz/cs503-project/models/idefics2-redcircle-fearless-morning-10-1650step/checkpoint-1650\"\n",
    "checkpoint_dir = \"llava-hf/vip-llava-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600eab7a8e6b415da07f323c126aa2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929ef1718265492ea214cc9377254c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b11168b1b24f84a99225d06cbd3aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76be5af26c94d3f8e1996124bb68db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0eb7377bbb14046a983f8fc3c63328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f053f2ebfa4d3db82eb73f04f818de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950eb2a366eb44d2a8bca0a5f957e563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/transformers/models/vipllava/configuration_vipllava.py:98: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f14d8bd7e488cb0df4451fa1c8dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7e341161eb4103b5e4cfefc30c1382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf034bfe7a049b1847fe0586cd49e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcaf13b233648b59d81e336d24ef476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e798b692a73433b886e511c3d692b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2553f77a4e284642a0e1ca258ce7a22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370a7af32ff4d449d8c210d688ad938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    do_image_splitting=False\n",
    ")\n",
    "\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        # target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = VipLlavaForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "else:\n",
    "    model = VipLlavaForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        # _attn_implementation=\"flash_attention_2\", # Only available on A100 or H100\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def vizualize_frames(image_paths):\n",
    "    y_view_mapping = {\"MIDDLE\": 1, \"LEFT\": 0, \"RIGHT\": 2}\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(48, 18))\n",
    "    for i, (image_view, image_path) in enumerate(image_paths.items()):\n",
    "        # image = Image.open(image_path)\n",
    "        image=copy.deepcopy(image_path)\n",
    "        _, x, y = f\"{image_view}_MIDDLE\".split(\"_\")[:3]\n",
    "        x_id = int(x == 'BACK')\n",
    "        axes[x_id][y_view_mapping[y]].imshow(image)\n",
    "        axes[x_id][y_view_mapping[y]].set_title(image_view)\n",
    "        axes[x_id][y_view_mapping[y]].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def process_scene(scene_id, scene):\n",
    "    samples = []\n",
    "    for frame_id, frame in scene['key_frames'].items():\n",
    "        image_paths = {view_name: view_path.replace(IMAGE_PATH_PREFIX, IMAGE_DIR) for view_name, view_path in frame['image_paths'].items()}\n",
    "        assert len(image_paths) == 6, \"not all views provided\"\n",
    "        question_id = 0\n",
    "        for question_type, questions in frame['QA'].items():\n",
    "            for question_info in questions:\n",
    "                question = question_info['Q']\n",
    "                answer = question_info['A'] if \"A\" in question_info else \"\"\n",
    "                sample_id = f\"{scene_id}_{frame_id}_{question_id}\"\n",
    "                question_id += 1\n",
    "                question_text = question\n",
    "                samples.append({\n",
    "                    \"id\": sample_id, #change key here from sample_id to id\n",
    "                    \"question_type\": question_type,\n",
    "                    \"question_text\": question_text,\n",
    "                    \"images\": image_paths,\n",
    "                    \"answer\": answer,\n",
    "                    \"tag\": question_info[\"tag\"]\n",
    "                })\n",
    "    return samples\n",
    "\n",
    "\n",
    "def process_dataset(data_path, output_path=None):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        dataset: Dict[str, str] = json.load(f)\n",
    "    samples = []\n",
    "    for scene_id, scene in tqdm(dataset.items()):\n",
    "        samples.extend(process_scene(scene_id, scene))\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(samples, f, indent=4)\n",
    "    return samples\n",
    "\n",
    "import re\n",
    "def objects_to_dict(question):\n",
    "    # get all objects in the question\n",
    "    objects = re.findall(r'<[^>]*>', question)\n",
    "    unique_objects = list(set(objects))\n",
    "    result = {}\n",
    "    for obj in unique_objects:\n",
    "        # Remove '<' and '>' and split by comma\n",
    "        parts = obj.strip('<>').split(',')\n",
    "        # The identifier seems to be the second element based on your example\n",
    "        identifier = parts[1]\n",
    "        # Coordinates are the last two elements\n",
    "        coordinates = [float(parts[2]), float(parts[3])]\n",
    "        # Check if the identifier already exists in the dictionary\n",
    "        if identifier in result:\n",
    "            # Append the new coordinates to the existing list\n",
    "            result[identifier].append(coordinates)\n",
    "        else:\n",
    "            # Otherwise, create a new list with the coordinates\n",
    "            result[identifier] = [coordinates]\n",
    "    # result will look like {'CAM_BACK': [[1088.3, 497.5]], 'CAM_FRONT': [[1043.2, 82.2]]}\n",
    "    return result\n",
    "\n",
    "def draw_circle(image_path,image_key, objects, colors=[\"red\"]):\n",
    "    image = load_image(image_path)\n",
    "    assert len(objects) <= len(colors)\n",
    "\n",
    "    if image_key in objects.keys() and bool(objects):\n",
    "        for coordinate, color in zip(objects[image_key], colors):\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            # Define the radius of the circle and the color\n",
    "            # Base on paper: we draw red circles over the images, with radius r = 0.06H and thickness t = 0.01H, where H is the shorter side of the image.\n",
    "            H= min(image.size)\n",
    "            radius = 0.06 * H\n",
    "            thickness = 0.01 * H\n",
    "            x = float(coordinate[0])\n",
    "            y = float(coordinate[1])\n",
    "            # Calculate the bounding box of the circle to be drawn\n",
    "            left_up_point = (int(x - radius), int(y - radius))\n",
    "            right_down_point = (int(x + radius), int(y + radius))\n",
    "            draw.ellipse([left_up_point, right_down_point], outline=color, fill=None, width=int(thickness))\n",
    "            #for checking center\n",
    "            # radius_center=10\n",
    "            # left_up_point = (int(x - radius_center), int(y - radius_center))\n",
    "            # right_down_point = (int(x + radius_center), int(y + radius_center))\n",
    "            # draw.ellipse([left_up_point, right_down_point],fill='blue')\n",
    "\n",
    "    return image\n",
    "\n",
    "def construct_for_viz(image_paths,images):\n",
    "    for i,key in enumerate(image_paths.keys()):\n",
    "        image_paths[key]=images[i]\n",
    "    return image_paths\n",
    "\n",
    "def eval_model(model, test_set, apply_visual_cue=True, apply_vb=True, verbose=False):\n",
    "    predictions = []\n",
    "    for llava_sample in tqdm(test_set):\n",
    "        image_paths = llava_sample['images']\n",
    "\n",
    "        if apply_visual_cue:\n",
    "            objects = objects_to_dict(llava_sample['question_text'])\n",
    "            # images = [load_image(image_path).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_path in image_paths.values()]\n",
    "            colors = [\"red\", \"blue\", \"black\", \"white\"]\n",
    "            images = [draw_circle(image_paths[image_key], image_key, objects, colors=colors).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_key in image_paths.keys()]\n",
    "        else:\n",
    "            images = [load_image(image_path).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_path in image_paths.values()]\n",
    "\n",
    "        question = llava_sample['question_text']\n",
    "        image_tokens = \" \".join([\"<image>\"] * len(images))\n",
    "        prompt = f\"A chat between a human driver and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: {image_tokens}\\n{question}\\n###Assistant:\"\n",
    "\n",
    "        if apply_vb:\n",
    "            raw_objects = re.findall(r'<[^>]*>', llava_sample['question_text'])\n",
    "            for object, color in zip(raw_objects, colors[:len(objects)]):\n",
    "                prompt = prompt.replace(object, f\"the object marked with {color} circle\")\n",
    "                prompt = prompt.replace(\"object the object\", \"the object\")\n",
    "        \n",
    "        inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "        if verbose:\n",
    "            image_viz = construct_for_viz(copy.deepcopy(image_paths),images)\n",
    "            vizualize_frames(image_viz)\n",
    "            print('objects:',objects)\n",
    "\n",
    "        # Generate\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "        predicted_text = generated_texts[0].split('\\n')[-1][len(\"###Assistant:\"):].strip()\n",
    "        # predicted_text = generated_texts[0]\n",
    "        prediction = copy.deepcopy(llava_sample)\n",
    "        prediction['gt'] = prediction['answer']\n",
    "        prediction['answer'] = predicted_text\n",
    "        predictions.append(prediction)\n",
    "        if verbose:\n",
    "            print(prompt)\n",
    "            print('Predicted:', predicted_text)\n",
    "            print('GT:', prediction['gt'])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10894.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = process_dataset(train_data_path)\n",
    "test_dataset = process_dataset(test_data_path)\n",
    "\n",
    "# pathlib.Path(vipllava_test_data_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "# with open(vipllava_test_data_path, \"w\") as f:\n",
    "#     json.dump(test_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [01:43<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_predictions = eval_model(model, test_dataset, apply_visual_cue=True, apply_vb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_path = \"/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/vip-llava/outputs/test-eval-vip-llava-7b-zero-shot-visual-cue-vb.json\"\n",
    "pathlib.Path(zero_shot_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(zero_shot_path, \"w\") as f:\n",
    "    json.dump(zero_shot_predictions, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs503",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
