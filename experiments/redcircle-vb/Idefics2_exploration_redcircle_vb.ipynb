{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d587931-23c6-4aba-9db3-9aaca6b2055f",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d73d13d-f066-43a0-9e67-f4f1301efcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate datasets peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b0985-8fa0-486f-ac67-7ec2638c6108",
   "metadata": {},
   "source": [
    "### Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294860c-9ff8-4295-9d88-bd60026b5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "with open(\"/mnt/nlpdata1/home/ismayilz/.wandb.key\", \"r\") as f:\n",
    "    wandb_key = f.read().strip()\n",
    "    \n",
    "wandb.login(key=wandb_key)\n",
    "wandb.init(\n",
    "    project=\"thinking-fast-and-furious\",\n",
    ")\n",
    "wandb_name=wandb.run.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220712f0-2373-459a-ba34-137696e94ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from transformers.image_utils import load_image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7044a9-b085-4c3f-bdd1-4be9e6e71910",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/samples'\n",
    "IMAGE_PATH_PREFIX = '../nuscenes/samples'\n",
    "IMAGE_SRC_X, IMAGE_SRC_Y = 1600, 900\n",
    "IMAGE_TGT_X, IMAGE_TGT_Y = int(IMAGE_SRC_X / 2.5), int(IMAGE_SRC_Y / 2.5)\n",
    "\n",
    "train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/v1_1_train_nus_ext_converted.json'\n",
    "test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/drivelm/challenge/test_eval.json'\n",
    "idefics_train_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-vb/data/nuscenes/train_idefics_redcircle_vb.json'\n",
    "idefics_test_data_path = '/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-vb/data/nuscenes/test_idefics_redcircle_vb.json'\n",
    "checkpoint_dir = \"/mnt/nlpdata1/home/ismayilz/cs503-project/models/idefics2-redcircle-vb/checkpoint-1000\"\n",
    "model_dir = \"HuggingFaceM4/idefics2-8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81418b8-2fc6-43a3-94e2-5d3b520dd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd487064449c4e909c20c4b896d00bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/nlpdata1/home/ismayilz/.cache/huggingface\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_dir,\n",
    "    do_image_splitting=False\n",
    ")\n",
    "\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "    # model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "else:\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\", # Only available on A100 or H100\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6e0d44-7fe7-4ab5-9738-47b0c9630988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def vizualize_frames(image_paths):\n",
    "    y_view_mapping = {\"MIDDLE\": 1, \"LEFT\": 0, \"RIGHT\": 2}\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(48, 18))\n",
    "    for i, (image_view, image_path) in enumerate(image_paths.items()):\n",
    "        # image = Image.open(image_path)\n",
    "        image=copy.deepcopy(image_path)\n",
    "        _, x, y = f\"{image_view}_MIDDLE\".split(\"_\")[:3]\n",
    "        x_id = int(x == 'BACK')\n",
    "        axes[x_id][y_view_mapping[y]].imshow(image)\n",
    "        axes[x_id][y_view_mapping[y]].set_title(image_view)\n",
    "        axes[x_id][y_view_mapping[y]].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def process_scene(scene_id, scene):\n",
    "    samples = []\n",
    "    for frame_id, frame in scene['key_frames'].items():\n",
    "        image_paths = {view_name: view_path.replace(IMAGE_PATH_PREFIX, IMAGE_DIR) for view_name, view_path in frame['image_paths'].items()}\n",
    "        assert len(image_paths) == 6, \"not all views provided\"\n",
    "        question_id = 0\n",
    "        for question_type, questions in frame['QA'].items():\n",
    "            for question_info in questions:\n",
    "                question = question_info['Q']\n",
    "                answer = question_info['A'] if \"A\" in question_info else \"\"\n",
    "                sample_id = f\"{scene_id}_{frame_id}_{question_id}\"\n",
    "                question_id += 1\n",
    "                question_text = question\n",
    "                samples.append({\n",
    "                    \"id\": sample_id, #change key here from sample_id to id\n",
    "                    \"question_type\": question_type,\n",
    "                    \"question_text\": question_text,\n",
    "                    \"images\": image_paths,\n",
    "                    \"answer\": answer,\n",
    "                    \"tag\": question_info[\"tag\"]\n",
    "                })\n",
    "    return samples\n",
    "\n",
    "\n",
    "def process_dataset(data_path, output_path=None):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        dataset: Dict[str, str] = json.load(f)\n",
    "    samples = []\n",
    "    for scene_id, scene in tqdm(dataset.items()):\n",
    "        samples.extend(process_scene(scene_id, scene))\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(samples, f, indent=4)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def convert_sample_to_idefics(sample):\n",
    "    idefics_sample = copy.deepcopy(sample)\n",
    "    question = sample[\"question_text\"]\n",
    "    user_message = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK_LEFT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_BACK_RIGHT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT_LEFT\"},\n",
    "                {\"type\": \"image\", \"text\": \"CAM_FRONT_RIGHT\"},\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    idefics_sample[\"user_message\"] = user_message\n",
    "    return idefics_sample\n",
    "\n",
    "\n",
    "def produce_idefics_dataset(samples, output_path=None):\n",
    "    idefics_samples = []\n",
    "    for sample in samples:\n",
    "        idefics_samples.append(convert_sample_to_idefics(sample))\n",
    "    if output_path:\n",
    "        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(idefics_samples, f, indent=4)\n",
    "            \n",
    "    return idefics_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c1997c8-a985-425a-805d-fe32d0f5c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def objects_to_dict(question):\n",
    "    # get all objects in the question\n",
    "    objects = re.findall(r'<[^>]*>', question)\n",
    "    unique_objects = list(set(objects))\n",
    "    result = {}\n",
    "    for obj in unique_objects:\n",
    "        # Remove '<' and '>' and split by comma\n",
    "        parts = obj.strip('<>').split(',')\n",
    "        # The identifier seems to be the second element based on your example\n",
    "        identifier = parts[1]\n",
    "        # Coordinates are the last two elements\n",
    "        coordinates = [float(parts[2]), float(parts[3])]\n",
    "        # Check if the identifier already exists in the dictionary\n",
    "        if identifier in result:\n",
    "            # Append the new coordinates to the existing list\n",
    "            result[identifier].append(coordinates)\n",
    "        else:\n",
    "            # Otherwise, create a new list with the coordinates\n",
    "            result[identifier] = [coordinates]\n",
    "    # result will look like {'CAM_BACK': [[1088.3, 497.5]], 'CAM_FRONT': [[1043.2, 82.2]]}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5c38c6-797b-4bd1-b90d-24856c0bdc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(image_path,image_key, objects, colors=[\"red\"]):\n",
    "    image = load_image(image_path)\n",
    "    assert len(objects) <= len(colors)\n",
    "\n",
    "    if image_key in objects.keys() and bool(objects):\n",
    "        for coordinate, color in zip(objects[image_key], colors):\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            # Define the radius of the circle and the color\n",
    "            # Base on paper: we draw red circles over the images, with radius r = 0.06H and thickness t = 0.01H, where H is the shorter side of the image.\n",
    "            H= min(image.size)\n",
    "            radius = 0.06 * H\n",
    "            thickness = 0.01 * H\n",
    "            x = float(coordinate[0])\n",
    "            y = float(coordinate[1])\n",
    "            # Calculate the bounding box of the circle to be drawn\n",
    "            left_up_point = (int(x - radius), int(y - radius))\n",
    "            right_down_point = (int(x + radius), int(y + radius))\n",
    "            draw.ellipse([left_up_point, right_down_point], outline=color, fill=None, width=int(thickness))\n",
    "            #for checking center\n",
    "            # radius_center=10\n",
    "            # left_up_point = (int(x - radius_center), int(y - radius_center))\n",
    "            # right_down_point = (int(x + radius_center), int(y + radius_center))\n",
    "            # draw.ellipse([left_up_point, right_down_point],fill='blue')\n",
    "\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deaabb30-7c12-435a-b8a8-88978fbdad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_for_viz(image_paths,images):\n",
    "    for i,key in enumerate(image_paths.keys()):\n",
    "        image_paths[key]=images[i]\n",
    "    return image_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a2ef9b-cdaf-44cf-9818-db8fbf26ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_set, verbose=False):\n",
    "    predictions = []\n",
    "    for idefics_sample in tqdm(test_set):\n",
    "        image_paths = idefics_sample['images']\n",
    "        objects = objects_to_dict(idefics_sample['question_text'])\n",
    "        colors = [\"red\", \"blue\", \"black\", \"white\"]\n",
    "        images = [draw_circle(image_paths[image_key], image_key, objects, colors=colors).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_key in image_paths.keys()]\n",
    "        \n",
    "        if verbose:\n",
    "            image_viz=construct_for_viz(copy.deepcopy(image_paths),images)\n",
    "            vizualize_frames(image_viz)\n",
    "            print('objects:',objects)\n",
    "        \n",
    "        prompt = processor.apply_chat_template(idefics_sample['user_message'], add_generation_prompt=True)\n",
    "        \n",
    "        raw_objects = re.findall(r'<[^>]*>', idefics_sample['question_text'])\n",
    "\n",
    "        for object, color in zip(raw_objects, colors[:len(objects)]):\n",
    "            prompt = prompt.replace(object, f\"the object marked with {color} circle\")\n",
    "            prompt = prompt.replace(\"object the object\", \"the object\")\n",
    "        \n",
    "        inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "        # Generate\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "        predicted_text = generated_texts[0].split('\\n')[-1][len(\"Assistant: \"):]\n",
    "        prediction = copy.deepcopy(idefics_sample)\n",
    "        prediction['gt'] = prediction['answer']\n",
    "        prediction['answer'] = predicted_text\n",
    "        predictions.append(prediction)\n",
    "        if verbose:\n",
    "            print(idefics_sample['question_text'])\n",
    "            print('Predicted:', predicted_text)\n",
    "            print('GT:', prediction['gt'])\n",
    "    return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18709197-47be-45de-967a-9dd4e50d2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 9157.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = process_dataset(train_data_path)\n",
    "test_dataset = process_dataset(test_data_path)\n",
    "\n",
    "# train_idefics_dataset = produce_idefics_dataset(train_dataset, output_path=idefics_train_data_path)\n",
    "test_idefics_dataset = produce_idefics_dataset(test_dataset, output_path=idefics_test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cca1dc-a1da-4b69-b377-6926279ffe7a",
   "metadata": {},
   "source": [
    "### Zero-shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a29ff74-3313-4ab7-90aa-536226f90bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "100%|██████████| 66/66 [04:21<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_predictions = eval_model(model, test_idefics_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e8a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/nlpdata1/home/ismayilz/cs503-project/thinking-fast-and-furious/experiments/redcircle-vb/outputs/test-eval-idefics2-8b-fine-tuned-redcircle-vb-1000step.json\"\n",
    "with open(path, \"w\") as f:\n",
    "     json.dump(zero_shot_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "205539c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../redcircle/outputs/test-eval-idefics2-8b-fine-tuned-redcircle-prime-music-8-500step.json\") as f:\n",
    "    orig_results = json.load(f)\n",
    "\n",
    "merged_results = []\n",
    "new_results_map = {}\n",
    "\n",
    "for res in zero_shot_predictions:\n",
    "    new_results_map[res[\"id\"]] = res\n",
    "\n",
    "for res in orig_results:\n",
    "    new_res = new_results_map.get(res[\"id\"])\n",
    "\n",
    "    if new_res:\n",
    "        new_res[\"raw_answer\"] = new_res[\"answer\"]\n",
    "        new_res[\"answer\"] = new_res[\"answer\"].strip()[0]\n",
    "        merged_results.append(new_res)\n",
    "    else:\n",
    "        merged_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1122271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/test-eval-idefics2-8b-fine-tuned-redcircle-prime-music-8-500step-one-shot.json\", \"w\") as f:\n",
    "    json.dump(merged_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5d1d7-1422-415c-a6cb-18e189b1d8a3",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940c40-3e4e-4049-b61f-12026f1e613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #new change to so that it's different scenes in train and test\n",
    "# from datasets import load_dataset\n",
    "# from sklearn.model_selection import train_test_split \n",
    "\n",
    "# data = load_dataset('json', data_files=idefics_train_data_path, split=None)\n",
    "# scene_ids = list(set(data['train']['scene_id']))\n",
    "# train_ids, val_ids = train_test_split(scene_ids, test_size=0.025)\n",
    "# train_dataset = data['train'].filter(lambda x: x['scene_id'] in train_ids)\n",
    "# val_dataset = data['train'].filter(lambda x: x['scene_id'] in val_ids)\n",
    "# print(f'Train set: {len(train_dataset)}, Val set: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c8d49f-e80e-44c9-9d05-894d9eaefa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 29450 examples [00:01, 17348.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('json', data_files=idefics_train_data_path, split=None)\n",
    "split_dataset = data['train'].train_test_split(test_size=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db613a3-f72b-49ed-9559-a6347b97305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVQADataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            objects = objects_to_dict(example['question_text'])\n",
    "            colors = [\"red\", \"blue\", \"black\", \"white\"]\n",
    "            sample_images = [draw_circle(example['images'][image_key], image_key, objects, colors=colors).resize((IMAGE_TGT_X, IMAGE_TGT_Y)) for image_key in example['images'].keys()]\n",
    "\n",
    "            #for make sure red circle is there\n",
    "            # image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\n",
    "            # vizualize_frames(image_viz)\n",
    "            # print('Question:',)\n",
    "            # print('objects:',objects)\n",
    "            \n",
    "            answer_text = example[\"answer\"]\n",
    "            answer_message = {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": answer_text}\n",
    "                    ]\n",
    "            }\n",
    "            user_message = example['user_message'][0]\n",
    "            question = user_message[\"content\"][-1][\"text\"]\n",
    "            raw_objects = re.findall(r'<[^>]*>', question)\n",
    "\n",
    "            for object, color in zip(raw_objects, colors[:len(objects)]):\n",
    "                question = question.replace(object, f\"the object marked with {color} circle\")\n",
    "                question = question.replace(\"object the object\", \"the object\")\n",
    "\n",
    "            user_message[\"content\"][-1][\"text\"] = question\n",
    "            messages = [user_message, answer_message]\n",
    "            text = self.processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "            texts.append(text.strip())\n",
    "            # print(text)\n",
    "            images.append(sample_images)\n",
    "\n",
    "        batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = self.image_token_id\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d1570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['images', 'answer', 'user_message', 'id', 'tag', 'question_text', 'question_type'],\n",
       "    num_rows: 737\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fd040ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m GVQADataCollator(processor)\n\u001b[0;32m----> 3\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m, in \u001b[0;36mGVQADataCollator.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m objects \u001b[38;5;241m=\u001b[39m objects_to_dict(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m [draw_circle(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][image_key], image_key, objects, colors\u001b[38;5;241m=\u001b[39mcolors)\u001b[38;5;241m.\u001b[39mresize((IMAGE_TGT_X, IMAGE_TGT_Y)) \u001b[38;5;28;01mfor\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#for make sure red circle is there\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# vizualize_frames(image_viz)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('Question:',)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('objects:',objects)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m answer_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m objects \u001b[38;5;241m=\u001b[39m objects_to_dict(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m [\u001b[43mdraw_circle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize((IMAGE_TGT_X, IMAGE_TGT_Y)) \u001b[38;5;28;01mfor\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#for make sure red circle is there\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# vizualize_frames(image_viz)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('Question:',)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('objects:',objects)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m answer_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m, in \u001b[0;36mdraw_circle\u001b[0;34m(image_path, image_key, objects, colors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_circle\u001b[39m(image_path,image_key, objects, colors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(colors)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m objects\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(objects):\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/transformers/image_utils.py:335\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image, timeout)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect format used for image. Should be an url linking to an image, a base64 string, a local path, or a PIL image.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[0;32m--> 335\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageOps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexif_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/PIL/ImageOps.py:685\u001b[0m, in \u001b[0;36mexif_transpose\u001b[0;34m(image, in_place)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexif_transpose\u001b[39m(image: Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;241m*\u001b[39m, in_place: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    If an image has an EXIF Orientation tag, other than 1, transpose the image\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03m    accordingly, and remove the orientation data.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m        image will be returned.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m     \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     image_exif \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mgetexif()\n\u001b[1;32m    687\u001b[0m     orientation \u001b[38;5;241m=\u001b[39m image_exif\u001b[38;5;241m.\u001b[39mget(ExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/PIL/ImageFile.py:305\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_end()\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m err_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_collator = GVQADataCollator(processor)\n",
    "\n",
    "batch = data_collator(torch.utils.data.Subset(split_dataset[\"test\"], list(range(20))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c74f84-afd4-4840-8a19-c7e291181152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=500,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    output_dir=checkpoint_dir,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=75,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"idefics-8b-redcircle-vb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aabdabd-41e5-4e0a-9b93-11b55419a025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "data_collator = GVQADataCollator(processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"], # You can also evaluate (loss) on the eval set, note that it will incur some additional GPU memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1a0c10-9cfe-4471-9438-593c3ea8171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "/home/ismayilz/.conda/envs/cs503/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 39/500 07:46 < 1:36:52, 0.08 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/transformers/trainer.py:2165\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2166\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mGVQADataCollator.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m objects \u001b[38;5;241m=\u001b[39m objects_to_dict(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m [draw_circle(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][image_key], image_key, objects, colors\u001b[38;5;241m=\u001b[39mcolors)\u001b[38;5;241m.\u001b[39mresize((IMAGE_TGT_X, IMAGE_TGT_Y)) \u001b[38;5;28;01mfor\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#for make sure red circle is there\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# vizualize_frames(image_viz)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('Question:',)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('objects:',objects)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m answer_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m objects \u001b[38;5;241m=\u001b[39m objects_to_dict(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m colors \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m sample_images \u001b[38;5;241m=\u001b[39m [\u001b[43mdraw_circle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresize((IMAGE_TGT_X, IMAGE_TGT_Y)) \u001b[38;5;28;01mfor\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#for make sure red circle is there\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# image_viz=construct_for_viz(copy.deepcopy(example['images']),sample_images)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# vizualize_frames(image_viz)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print('Question:',)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print('objects:',objects)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m answer_text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mdraw_circle\u001b[0;34m(image_path, image_key, objects, colors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_circle\u001b[39m(image_path,image_key, objects, colors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 2\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objects) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(colors)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_key \u001b[38;5;129;01min\u001b[39;00m objects\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(objects):\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/transformers/image_utils.py:316\u001b[0m, in \u001b[0;36mload_image\u001b[0;34m(image, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(requests\u001b[38;5;241m.\u001b[39mget(image, timeout\u001b[38;5;241m=\u001b[39mtimeout)\u001b[38;5;241m.\u001b[39mcontent))\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(image):\n\u001b[0;32m--> 316\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:image/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/cs503/lib/python3.10/site-packages/PIL/Image.py:3256\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3254\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3256\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3258\u001b[0m preinit()\n\u001b[1;32m   3260\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc0ee3-d7da-48b0-9ddc-cab15346b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs\n",
    "finetuned_model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    \"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/models/idefics_redcircle/wandb_name/checkpoint-500\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config if USE_QLORA else None,\n",
    ")\n",
    "_=eval_model(finetuned_model, test_idefics_dataset[:10], verbose=True)\n",
    "fine_tune_predictions_path = f\"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/baseline/experiments/eea/outputs/test-eval-idefics2-8b-fine-tunned-{wandb_name}_redcirecle.json\"\n",
    "fine_tune_predictions = eval_model(model, test_idefics_dataset)\n",
    "with open(fine_tune_predictions_path, \"w\") as f:\n",
    "     json.dump(fine_tune_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8971f4-df3d-4c67-803c-55abc03d9d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
