{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171eccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8971f4-df3d-4c67-803c-55abc03d9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 696/696 [00:00<00:00, 2178.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from modeling import process_dataset, produce_idefics_dataset\n",
    "\n",
    "train_data_path = '/mnt/u14157_ic_nlp_001_files_nfs/nlpdata1/home/ismayilz/cs503-project/data/train/nuscenes/v1_1_train_nus_ext_converted.json'\n",
    "\n",
    "train_dataset = process_dataset(train_data_path, apply_context=\"chain\")\n",
    "train_idefics_dataset = produce_idefics_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b7a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoProcessor\n",
    "from modeling import get_objects, parse_object_ref\n",
    "\n",
    "model_dir = \"HuggingFaceM4/idefics2-8b\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_dir,\n",
    "    do_image_splitting=False\n",
    ")\n",
    "\n",
    "# with open(\"./data/nuscenes/train_idefics_redcircle_vb_chain.json\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "from modeling import GVQADataCollator\n",
    "\n",
    "# c = GVQADataCollator(processor)\n",
    "\n",
    "with open(\"data/nuscenes/train_idefics_redcircle_vb_chain_od.json\") as f:\n",
    "    train_idefics_dataset = json.load(f)\n",
    "\n",
    "for sample in train_idefics_dataset:\n",
    "    question_text = sample[\"user_message\"][0][\"content\"][-1][\"text\"]\n",
    "    question = question_text\n",
    "    # context = None\n",
    "\n",
    "    # if \"Task:\" in question_text:\n",
    "    #     parts = question_text.split(\"Task:\")\n",
    "    #     context = parts[0].strip()\n",
    "    #     question = parts[1].strip()\n",
    "\n",
    "    question_objects = get_objects(question)\n",
    "    colors = [\"red\", \"blue\", \"black\", \"white\", \"green\", \"yellow\", \"grey\", \"orange\"]\n",
    "    assert len(question_objects) <= len(colors)\n",
    "    \n",
    "    try:\n",
    "        for object in question_objects:\n",
    "            parse_object_ref(object)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(object)\n",
    "\n",
    "# _ = c(train_idefics_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845453a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<c1,CAM_BACK,1088.3,497.5>', '<c2,CAM_BACK,864.2,468.3>', '<c3,CAM_FRONT,1043.2,82.2>', '<c1,CAM_BACK,1088.3,497.5>', '<c1,CAM_BACK,1088.3,497.5>', '<c1,CAM_BACK,1088.3,497.5>', '<c2,CAM_BACK,864.2,468.3>', '<c2,CAM_BACK,864.2,468.3>', '<c3,CAM_FRONT,1043.2,82.2>']\n",
      "{'CAM_BACK': [[1088.3, 497.5], [864.2, 468.3]], 'CAM_FRONT': [[1043.2, 82.2]]}\n"
     ]
    }
   ],
   "source": [
    "text = \"Context:\\nQ:What are the important objects in the current scene? Those objects will be considered for the future reasoning and driving decision.\\nA:There is a brown SUV to the back of the ego vehicle, a black sedan to the back of the ego vehicle, and a green light to the front of the ego vehicle. The IDs of these objects are <c1,CAM_BACK,1088.3,497.5>, <c2,CAM_BACK,864.2,468.3>, and <c3,CAM_FRONT,1043.2,82.2>.\\nQ:What is the moving status of object <c1,CAM_BACK,1088.3,497.5>? Please select the correct answer from the following options: A. Stopped. B. Reverse parking. C. Turn left. D. Back up.\\nA:C\\nQ:What is the object <c1,CAM_BACK,1088.3,497.5>?\\nA:Brown SUV\\nQ:What is the bounding box and the category of the Brown SUV in back of the ego vehicle?\\nA:966.6,403.3,1224.1,591.7 Vehicle\\nQ:What is the center coordinate of the Brown SUV in back of the ego vehicle?\\nA:1088.3,497.5\\nQ:What is the status of the Brown SUV (<c1,CAM_BACK,1088.3,497.5>)?\\nA:Moving\\nQ:What is the object <c2,CAM_BACK,864.2,468.3>?\\nA:Black sedan\\nQ:What is the bounding box and the category of the Black sedan in back of the ego vehicle?\\nA:816.7,431.6,917.2,505.0 Vehicle\\nQ:What is the center coordinate of the Black sedan in back of the ego vehicle?\\nA:864.2,468.3\\nQ:What is the status of the Black sedan (<c2,CAM_BACK,864.2,468.3>)?\\nA:Moving\\nQ:What is the object <c3,CAM_FRONT,1043.2,82.2>?\\nA:Green light\\nQ:What is the bounding box and the category of the Green light in front of the ego vehicle?\\nA:676.4,0.0,1452.6,171.5 Traffic element\\nQ:What is the center coordinate of the Green light in front of the ego vehicle?\\nA:1043.2,82.2\\nTask:\\nWhat object should the ego vehicle notice first when the ego vehicle is getting to the next possible location? What is the state of the object that is first noticed by the ego vehicle and what action should the ego vehicle take? What object should the ego vehicle notice second when the ego vehicle is getting to the next possible location? What is the state of the object perceived by the ego vehicle as second and what action should the ego vehicle take? What object should the ego vehicle notice third? What is the state of the object perceived by the ego vehicle as third and what action should the ego vehicle take?\"\n",
    "from modeling import get_objects, objects_to_dict\n",
    "\n",
    "objects = get_objects(text)\n",
    "dict_o = objects_to_dict(objects)\n",
    "print(objects)\n",
    "print(dict_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc54e317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from modeling import load_model\n",
    "\n",
    "model_dir = \"HuggingFaceM4/idefics2-8b\"\n",
    "model = load_model(model_dir, eval_mode=False, use_lora=False, use_qlora=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b9d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from modeling import GVQADataCollator, load_processor\n",
    "import json\n",
    "\n",
    "with open(\"./data/nuscenes/train_idefics_redcircle_vb_chain_od.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "model_dir = \"HuggingFaceM4/idefics2-8b\"\n",
    "processor = load_processor(model_dir)\n",
    "\n",
    "collator = GVQADataCollator(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf1eb803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1430])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "batch = collator(data[130:160])\n",
    "print(batch[\"input_ids\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6943cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/966 [00:25<6:52:47, 25.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 5\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mcollator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     context_lengths\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/thinking-fast-and-furious/experiments/redcircle-vb/modeling.py:408\u001b[0m, in \u001b[0;36mGVQADataCollator.__call__\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    405\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(text\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    406\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(sample_images)\n\u001b[0;32m--> 408\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# added user message masking \u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/models/idefics2/processing_idefics2.py:231\u001b[0m, in \u001b[0;36mIdefics2Processor.__call__\u001b[0;34m(self, text, images, image_seq_len, padding, truncation, max_length, is_split_into_words, add_special_tokens, return_tensors)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Load images if they are URLs\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     images \u001b[38;5;241m=\u001b[39m [[load_image(im) \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m sample] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m--> 231\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mupdate(image_inputs)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/models/idefics2/image_processing_idefics2.py:529\u001b[0m, in \u001b[0;36mIdefics2ImageProcessor.preprocess\u001b[0;34m(self, images, do_convert_rgb, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_pad, do_image_splitting, return_tensors, input_data_format, data_format)\u001b[0m\n\u001b[1;32m    526\u001b[0m     images_list \u001b[38;5;241m=\u001b[39m [[convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m images_list \u001b[38;5;241m=\u001b[39m [[to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[1;32m    532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/models/idefics2/image_processing_idefics2.py:529\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    526\u001b[0m     images_list \u001b[38;5;241m=\u001b[39m [[convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m images_list \u001b[38;5;241m=\u001b[39m [[to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[1;32m    532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/models/idefics2/image_processing_idefics2.py:529\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    526\u001b[0m     images_list \u001b[38;5;241m=\u001b[39m [[convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m images_list \u001b[38;5;241m=\u001b[39m [[\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images] \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m images_list]\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images_list[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[1;32m    532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/transformers/image_utils.py:176\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_numpy(img)\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/PIL/Image.py:696\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    694\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m/anaconda/envs/tff/lib/python3.10/site-packages/PIL/Image.py:776\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    773\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in tobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m--> 776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_lengths = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(data)//100+1)):\n",
    "    batch = collator(data[i:i+100])\n",
    "    context_lengths.append(batch[\"input_ids\"].shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
