{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import load_and_resize_images, vizualize_frames, TAGGED_CHAT_TEMPLATE\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import copy\n",
    "import json\n",
    "import argparse\n",
    "from termcolor import colored\n",
    "from datasets import load_dataset\n",
    "\n",
    "def eval_model(model, processor, test_set, verbose=False):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    device = model.device\n",
    "    for idefics_sample in tqdm(test_set):\n",
    "        image_paths = idefics_sample['images']\n",
    "        images = load_and_resize_images(idefics_sample)\n",
    "        if verbose:\n",
    "            vizualize_frames(image_paths)\n",
    "    \n",
    "        prompt = processor.apply_chat_template(idefics_sample['user_message'], add_generation_prompt=True, chat_template=TAGGED_CHAT_TEMPLATE)\n",
    "        inputs = processor(text=prompt, images=images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "        # Generate\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "        generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "        predicted_text = generated_texts[0].split('\\n')[-1][len(\"Assistant: \"):]\n",
    "        prediction = copy.deepcopy(idefics_sample)\n",
    "        prediction['gt'] = prediction['answer']\n",
    "        prediction['answer'] = predicted_text\n",
    "        predictions.append(prediction)\n",
    "        if verbose:\n",
    "            print(colored(idefics_sample['question_text'], 'blue'))\n",
    "            print(colored('Predicted:', 'blue'), predicted_text)\n",
    "            print(colored('GT:', 'blue'), prediction['gt'])\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/experiments/idefics2/models/ignore_index_added/checkpoint-200\"\n",
    "test_data_path='/home/cchang/CS503_VisualIntelligence/thinking-fast-and-furious/experiments/redcircle/data/nuscenes/test_ext_idefics_redcircle.json'\n",
    "prediction_data_path='/home/jessica/EPFL/Course/CS503-VisualIntelligence/Project/thinking-fast-and-furious/experiments/idefics2/outputs/test-eval-idefics2-8b-ignore-index-added-200step.json'\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"HuggingFaceM4/idefics2-8b\",\n",
    "    do_image_splitting=False\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "# Create inputs\n",
    "finetuned_model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "test_idefics_dataset  = load_dataset('json', data_files=test_data_path, split='train')\n",
    "\n",
    "predictions = eval_model(finetuned_model, processor, test_idefics_dataset[:10], verbose=True)\n",
    "with open(prediction_data_path, \"w\") as f:\n",
    "    json.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
